              ============================================
             ||Chines Poetry Geberate Algorithim Research||
              ============================================

                              Author     : ChenLiangbo
                              started at : 2017.4.20

[Aim]
----poet style poem,Li Taibai
----Specific title of Song imbamic


=========================================================
[1]Chinese Poetry Generation with Recurrent Neural Networks
=========================================================
首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：
  (1) Convolutional Sentence Model（CSM）：CNN模型，用于获取一句话的向量表示。
  (2) Recurrent Context Model(RCM)    RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。
  (3) Recurrent Generation Model(RGM)：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。

【诗歌自动生成方法分类】--[2]
1，based on rules and templates
2，based on various genetic algorithms
3，based on various statistical machine translation，SMT，主要用于生成对联
4，based on summarization
5，deep learning methods 

【诗歌自动生成评价指标】--[2]
1，fluency，a poem must be read fluently
2，meaningful，a poem must intentionally convey some conceptual message that
   is meaningful under some interpretation
3，a poem must exhibit features that distinguishes it from non-poetic text
4，coherent，a poem should discuss some focused topics


=======================================================================
[2]Chinese Song Iambics Generation with Neural Attention-based Model
=======================================================================
基于attention的encoder-decoder框架,将历史已经生成的内容作为源语言序列，将下
一句要生成的话作为目标语言序列。需要用户提供第一句话，然后由第一句生成第二句
第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌
基于Attention机制配合,LSTM，可以学习更长的诗歌，同时在一定程度上，可以提高前后语义的连贯性

Sequence -> Encoder -> Hidden ->Decoder -> Sequence
(x1,x2)		LSTM   	   (h1,h2,)	(LSTM)		(h1,h2,..)
End of generation: Global Once,Global Always
Dataset: same as ref1
----train: 216 Beauty Yu,349 Butterflies Love Flowers
----test : 21,35
----evaluate: 18 experts
Measurements: Poetic,meaningful,fluency,related
Experiments:
----First Line as input 

===========================================================================
[3]Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema
===========================================================================
Main Ideas:
----Given intents,generate poems,iPoet
----polish,like human author modify poems

Input:Key Words  K = {k1,k2,..,} ki = {c1,c2,c3..,}
Output:Poem      
P = [c(1,1),c(1,2),...c(1,n),
     c(2,1),c(2,2),...c(2,n),
     ...			...
     c(m,1),c(m,2),...c(m,n)]

    n = 5 ,7
    m = 4, 8

Method
----Vectorize keyWords -> Vector (1,128)
----Encoder   RNN
----Decoder   RNN
----Polish    
----Stop      iterations = 10 or similarity of cosine < 0.5

Experiments
----Dataset : Tang Poems,Song Poems,61960  (5,7  -- 4,8)
----Intents : Title, Poems :Content
----Train   : 2000 , Valiations : 1000
----Loss    : corss entropy
----Conv 3x3, MaxPool,pad = 0
----batch   : 100
----lr      : 0.8

Measurement
----Computer
    ----Perplexity
    ----BELU
----Human
    ----Fluency
    ----Poetiness
    ----Coherence
    ----Meaning

====================================================================
[4]Chinese Poetry Generation with Planning based Neural Network
====================================================================
思想：根据写作意图生成每一行的关键字，根据关键字生成每一句

优点：
  （1）格式正确，写作主旨明确
  （2）写作意图可以是关键词，也可以是一句话，甚至还可以是一段文字描述
  （3）不仅可以从给定的诗词数据集中学习到知识，还可以从额外不相关的资源中学到知识
  （4）给定的关键词可以是现代汉语的没描述方式，而不是需要使用古汉语中专有的描述方式
  （5）给定某些关键信息，可以从维基百科提取关键词构成每一句的topic，然后据此生成每一句

实现：
  （1）使用poem planning model根据给定的关键词生成每一句的核心topic
  （2）根据每一句的topic一行一行的生成全诗
  （3）encoder-decoder model  with RNN

输入：作者给定的写作意图，可以是关键词，一句话，一段文章
过程：Poem Planning and Poem Generation

写作意图 -> 提取关键词  -> 关键词扩展  -> 每行主题 -> 每句诗

诗 N行 {l1,l2,l3,...,}
词 N个 {k1,k2,k3,...,}
对于Poem Generation Stage 每一句诗由当前关键词以及前面的诗级联作为输入生成
由此，每一句诗的生成都和关键词相关，同时也和前面所有的句子相关

[算法]
----Poem Planning
  （1）Keyword Extraction----TextRank algorithm 算法分析输入句子中每个词语的重要性
  （2）Keyword Expansion： query Q is too short to extract enough keywords
  ----Short:Recurrent Neural Network Language Model  predict the subsequent keywords according to the preceding sequence of keywords：argmaxk P(ki|k1:i−1) 
  ----For Long:Knowledge-based method ,extra knowledge sources,Given a keyword ki, the key idea of the method is to find some words that can best describe or interpret ki,
  use the encyclopedia entries as the source of knowledge to expand new keywords from ki

----Poem Generation
  (1)generated line by line. Each line is generated by taking the keyword specified by the Poem Planning model and all the preceding text as input
  (2)sequence-to-sequence mapping problem: the keyword specified by the Poem Planning model and the previously generated text of the poem
  (3)attention based RNN encoder-decoder:support multiple sequences as input


[Experiment]
--data: quatrain,4 x 5 or 4 x 7, from the Internet
--dataset:total 76,859 quatrains,2,000 poems for validation, 2,000 poems for testing
--segment:  CRF based word segmentation,calculate the TextRank score for every word,selected as the keyword for the line,xtract a sequence of 4 keywords for
every quatrain
--training: proposed attention based RNN enc-dec model
            vocabulary----6000
            embedding dimensionality----512,initialized by word2vec
            RNN units:512, initialized uniform distribution with support [-0.08,0.08]
            optimizer:AdaDelta algorithm
            minibatch:128
            final model is selected according to the perplexity on the validation se
--Evaluation:
    Poeticness
    Fluency
    Coherence
    Meaning
--Baselines
    SMT
    RNNLM
    RNNPG:RNN-based Poem Generator,standard RNNLM and then all the other lines are generated iteratively 
    ANMT:Attention based Neural Machine Translation method

--Results
    (1)compare to already exists poem generating system or method
    (2)compare to real poem by peot with this method,same subject

------------------------------------------------------------------------------
|I can not tell whatever is generated by machine from written by human poet!!!|
-------------------------------------------------------------------------------


===============================================
[5]Generating Topical Poetry 
===============================================
思想：首先制作一个比较大的词典，押韵词库，根据给定的主题，生成一系列相关的词，网络首先选择韵词作为每句的最后一个词，然后根此生成全诗

[Vocabulary]
--英文诗词，可以扩展到其它格式的诗词，根据音节提取出模式，给模式编码


[Topically Related Words and Phrases]
--scored list of 1000 words/phrases

[Choosing Rhyme Words]
--Strict Rhyme, CMU pronunciation dictionary
--Slant Rhyme
--Non-Topical Rhyming Words

[Constructing FSA of Possible Poems]
--choosing rhyme words
--encodes all word sequences 


[Path extraction through FSA with RNN]

[知乎，深度学习生成诗词综述]
http://qingmang.me/articles/-2609457114363059733

=====================================================================
[6]SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient
=====================================================================



=================================================================
[7]Efficient Estimation of Word Representations in Vector Space
=================================================================
目标：使用神经网络学习算法，根据语料库进行训练，将词映射到50-100维度的向量空间，将单词转换为连续的向量
单词向量化方法：神经网络语言模型-前馈网络

[Feedforward Neural Net Language Model]
--model:input, projection, hidden and output layers,在输入层，输入一个1xV向量，映射到N维，v是vocabulary大小，然后连接一个 N × D 的映射层，计算量过大，可以使用哈夫曼树编码方式对输出结果进行编码，将计算开支减小到一半


[Recurrent Neural Net Language Model] 卷及神经网络语言模型
--model :input, hidden and output layer


[New Log-linear Models] ---minimize computational complexity
----Continuous Bag-of-Words Model:CBOW
----Continuous Skip-gram Model

使用提出的两种模型对单词编码维向量的质量进行了对比，这种对比主要在于相似的单词编码之后再向量空间有比较大的相似性


===========================================================================================
[8]Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation
===========================================================================================
内容：人机对话，首先使用PMI生成名词作为对话的主要关键词，然后使用encoder-decoder model生成包含关键词的回复

主要思想：使用seq2BF模型代替seq2seq模型，主要使用前馈和反馈模型，根据第一个词生成回复的一句话，所选定的关键词会随机地出现在句子的任何地方

[Keyword Prediction]
--pointwise mutual information(PMI)：


======================================================================================
[13]Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation
=======================================================================================
内容：使用两个RNN来对序列数据进行编码解码处理，一个RNN将序列编码到固定长度的向量，另外一个RNN将向量解码到另一个序列，目的是计算在给定序列条件下另外一个序列发生的条件概率。训练网络最大化此条件概率，训练好的网络有两方面的应用，一是给定序列生成新的序列，另一个是判断两个序列的评分（相似度）
实验：英语-法语翻译，WMT’14 

[encoder]
X = (x1; x2; : : : ; xN) 
Y = (y1; y2; : : : ; yM)

每一个词都是序列，one-hot向量 -在词库上的表示，将词库排列，出现的地方为1，其余地方为0，非常稀疏
每个序列都嵌入到500维的向量空间
encoder包括1000个隐含单元

[decoder]
--直接接在encoder的隐含状态state上，最后使用softmax输出，将结果映射到了一个单词上

例子：
vocabulary_x = ['i',"am","a","student","teacher","doctor"]
vocabulary_y = ["是","我","一个","老师","医生","学生",]

word_x = ['student']  -> word_y = ["学生"]
x = [0,0,0,1,0,0] 
y = [0,0,0,0,0,1]

[model]
x -> embedding(512) ->encoder(RNN 1000) -> decoder(RNN 1000) -> softmax(arg max(yt)) 

句子的表示就谁生成的512维的向量在空间上的分布具有某种相关性


================================================================================
[14]Sequence to Sequence Learning with Neural Networks
================================================================================
内容：使用seq2seq模型实现翻译，LSTM 作为encoder和decoder，英语-法语翻译，数据集 WMT-14
贡献：
  (1) 序列到序列的语言模型
  (2) 将输入序列逆序可以对长句子有更好的翻译效果


==================================================================
[15]A Neural Conversational Model
===================================================================
[Main Idea] 
----machine conversation,
----predicting the next sentence given the previous sentence or sentences in a conversation,
----producing answers given by a probabilistic model trained to maximize the probability of the answer given some contex
----usage: machine translation, question/answering, and conversations


[Advantage]
----end-to-end and thus requires much fewer hand-crafted rules,the model can give a technicol answer
----generating fluent and accurate replies to conversations

dataset : IT helpdesk dataset,queries and reponses

[model] ---- seq2seq
----Input : 'ABC' + 'EOS'
----Output: 'WXYZ' + 'EOS'
----Model : 'ABC' -> 'WXYZ'


[dataset]
----One : closed-domain IT helpdesk troubleshooting dataset
----Two : open-domain movie transcript dataset,OpenSubtitles

[Experiments]
----IT Helpdesk Troubleshooting experiments
  LSTM(One layer,1024 units),SGD
  VOCABULARY : 20K words
  perplexity : 8 (n-gram:18)
  remember facts,understand contexts
  model can generalize to new question


======================================================================================
[16]Iterative Alternating Neural Attention for Machine Reading
======================================================================================
思想：使用可替代迭代机制来解决机器阅读理解任务，比如完形填空。完形填空的问题是在一个完整的句子当中删去一些词语生成的，所提出的模型被称为 neural attention-based inference model，模型使用一个RNN将数据读入，然后使用一种迭代机制寻找，document，query，word执念的关系，
数据可以描述为这样的关系对(Q; D; A; a)

[Alternating Iterative Attention]
encoding phase:  compute a set of vector representations
    X = (x1; : : : ; x|Xj)  document or query,vocabulary V
    Each word is represented by a continuous word embedding
    Encoder : GRU
inference phase: 寻求文档和query之间的关系以确保腿短结果有证可寻
iterative process：at each iteration, alternates attentive memory accesses to the 
    query and the document
prediction phase ： maximize the probability of the correct answer

[Experiments]
  batch_size: 32
  optimizer : SGD with Adam
  norm      : gradient > 5, gradient = 0.5*gradient
  lr        : lr = 0.8*lr if accuracy of validation not change
  embedding : 256 - 384
  decoder   : 128 - 256
  GRU       : 256 - 512
  dropout   : 0.2 of inference GRU

===========================================================================
[17]Neural Machine Translation by Joint Learning to Align and Translate
===========================================================================
内容：定长的序列模型限制了魔性的使用，所以文中使用不定长的序列来实现机器翻译，模型根据输入的不同，自动寻找与输出相关的部分，然后生成翻译的结果输出。模型在英语-法语翻译任务上进行得到测试。文中提出的模型是对传统encoder-decoder魔性的扩展，是模型具有联合翻译的能力。每一个时刻，模型产生一个单词输出的额时候，都会在输入的源句里面寻找最相关的信息，所以产生的输出不仅和当前已经生成的输出结果相关，还输入中的有关信息相关。
创意：模型的创意在于不是一次性间输入序列encode到一个固定长度的向量，而是将输入序列编码到一系列的向量序列中，在解码的时候从这一系列的序列向量中选择一部分，这样的方式可以解决原来模型中不管输入序列用没有用都必须一次性选择，然后对输出必有影响的局限。于是现在变成了，输出是收到输入序列影响的，但是这种影响的重要性被分成了很多小的子部分，在每次输出的时候收到什么成分影响，影响的大小如何完全由模型来自动决定。

[model]
encoder: a bidirectional RNN,这种RNN包括了前向和反向，前向的时候按照顺序读入输入序列，反向的时候将输入序列逆序读入，都计算隐含状态。
decoder: a decoder that emulates searching through a source sentence during decoding a translation (speech recognition perform well)

[Experiments]
--dataset : English-to-French translation,  ACL WMT ’14
--


=====================================================================================
[18]Generating Chinese Classical Poems with Statistical Machine Translation Models
=====================================================================================
内容：提出一种基于概率的方法生成中国古诗，以及一种评价古诗的方法，算法接收一系列给定的关键词，代表着写作意向，然后一句一句地顺着生成古诗，这个算法称为统计机器翻译。一个模型生成一句，使用一个额外的模型来加强句子之间的相关性，文中的算法时使用模板匹配生成第一句，然后使用统计翻译模型有第一句生成第二句，第二句生成第三句，第三句生成第四局。文中的写作意图也是是用几个关键字，这些关键词限制在《诗学含英》所归纳整理的大类中的词，根据关键词，根据诗歌中平平zhezhe对仗规律选择相应的词放在对应的位置上

模板匹配法：用户选定韵节词汇，给定关键词，于是系统从现有的词库中选取相应的词汇构成一首诗，这样生成的满足了押韵，但是没有一个中心，稻香老农系统辨识使用这样的方法生成古诗词的


==========================================================================
[19]Chameleons in imagined conversations: A new approach to understanding
coordination of linguistic style in dialogs
===========================================================================
[Main]




===========================================================================
[20]Dropout: A Simple Way to Prevent Neural Networks from Overfitting
===========================================================================
内容：dropout就是随机地在网络中去除部分神经元以及他们之间的链接，这样可以降低网络中神经元之间的相互依赖性。在训练的时候屏蔽掉一些神经元将神经网络变瘦，在测试的时候恢复为胖网络。dropout的有点在于有效抑制过拟合的同时，也能够很好的训练出不同的网络，而且不同的网络在同样的数据上以相同的目标函数进行训练。


结论：在神经网络中dropout的p表示随机打开网络的概率，p=0.5表示，在这一层的网络中有50%的神经元会选择随机打开，另外50%的神经元随机关闭。一般而言，对于输入层的弃权通常设置p=0.8对于隐含层的弃权通常设置p=0.5.弃权不仅可以抑制网络的过拟合，同时还可以提高网络的表现性能，增加网络的稀疏性。



==============================================================================
[21]Subword Language Modeling with Neural Networks
==============================================================================
摘要：在单词级别和字符级别上研究几种语言模型在几个任务上的表现：前馈网络和n-gram模型。然后提出了一种sub-word 模型，将单次级别和字符级别的优势结合起来。在RT04语音识别任务上，前馈网络的模型比n-gram模型参数更少，使用sub-word模型后，还可以降低。

引言：使用单词级别的模型都会面临一个词典溢出的情况，对于这种情况可以使用一个subword-level的模型，溢出的词就交给subword-level模型处理。虽然表现很好，但是缺乏原子性，原子单元不唯一，还有对于某些语言溢出率很高，即使词典足够大。subword-level的溢出率是0，而且在很多地方表现比word-level好，使用subword-level可以对模型进行压缩，减少参数，降低训练时间。


=====================================================================================
[22]How Many Trees in a Random Forest
=====================================================================================
摘要：随机森林虽然功能强大，但是却没有明确的说法一个森林里到底需要多少棵树。增加树的数量会使得结果的表现型变好，但是这样也会造成计算开销增大。文中的观点是随着树的增加，并不一定增加表现性能


引言：机器学习一个基本的想法是将很多歌简单的分类器进行组合得到的结果很可能比单一强大的分类器效果好。这些模型就是boost,bag,Random Forest。随机森林在表现上比boosting和bagging好，计算速度也快，对噪声的兼容性很高，同事不容易出现过拟合的情况。



=============================================================================
[23]Image Super-Resolution Using Deep Convolutional Networks
=============================================================================
【内容】
使用一种端到端的卷积神经网络来实现超精度，有这样的优点：网络简单，计算速度快。

【思想】
辨率低的图像首先使用双线性插值将图像的尺寸变为需要的尺寸，这也是唯一的预处理。预处理之后的图像是Y，需要得到高分辨率的图像X，Y和X有相同的尺寸，这一种转换关系为 X =F(Y)。F包含了三个方面的操作
（1）补丁提取和表示，从低精度的图片中提取出补丁，生成高维的向量这些高维向量包含一些列的特征映射，特征映射的数量和高维向量的维度相等。
（2）非线性映射将取得的高维向量非线性映射到另一个高维向量，生成的高维向量就代表高精度图片的映射，同时也包括另一系列特征映射。
（3）重构，将第二步得到的高维向量进行组合得到最终所需要的高分辨率图片，这一张图片便和目标X相似。

【实现】
(1)补丁提取和表示（Patch Extraction and Representation）
第一层：F1(Y) = max(0,W1*Y + B1)      
W1 -> filters(c,f1,f1,n1)     c通道数量，f1卷积核大小，n1表示卷积核数目
B1 -> biases
*  -> convolution operation

(2)非线性映射(Non-Linear Mapping)
第二层：F2(Y) = max(0,W2*F1(Y) + B2)
W2 -> filters(n2,f2,f2,n2)

(3)重构(Reconstruction)
第三层：F3(Y) = W3*F2(Y) + B3
W3 -> filter (c,f3,f3,n3)


[参数设置]
f1 = 9, f2 = 1, f3 = 5 
n1 = 64,n2 = 32
[稀疏编码方法]

[训练]
数据：输入低分辨率图片，输出高分辨率图片
损失：均方根损失函数MSE
算法：SGD，BP
学习率：前两层0.0001，最后一层使用0.00001，最后一层使用比较小的学习率模型更容易收敛
输入：Xi (f*f*c)
细节：0边界，cuda-convnet ， Caffe 

[实验]
数据集：ILSVRC 2013 ImageNet detection training partition 低分辨率33，
结论：对于超分辨率的图像处理问题，存在这样几个事实：在网络结构固定的时候，滤波器越多效果越好，滤波器的卷积核越大效果越好，但是两者都是速度变慢。并不是层数越多越好，因为纯卷积，没有池化也没有全连接，因此对初始条件和超参数的选择都比较敏感，层数越多不容易收敛。


================================================================================
[24]Learning to Protect Communications with Adversarial Neural Cryptography
================================================================================
[摘要]神经网络是否可以使用密码来保护相互之间的通讯，两个神经网络Bob和Alice通信，Eve窃取两者之间的交流。
[引言]神经网络不仅知道怎么样加密，还知道什么该加密

[对称加密]
Alice和Bob希望可靠通信，Eve想偷听他们的通信
Alice: P + K -> C   (P,plaintext,C ciphertext,K,shared key)  加密（通过秘钥）
Bob: C + K -> Pb   (Bob)                                     解密（通过秘钥）
Eve: C -> Pe       (Eve)                                     解密（破解密码）
One fresh key for a plaintext P   (一个明文使用一个共享密码来加密)

网络结构：加密是一个全连接层，主要是用于学习异或加密手段，所用的数据都是浮点型，16位，明文使用高斯分布产生，密钥随机选取
程序：tensorflow/models/adversarial_crypto/