				==================================
				||		深度学习生成古诗词研究	||
				==================================

核心思想：论文[4]，首先给定一句话(短语，句子，短文)从这句话中提取出关键词，根据对应的每一个关键词生成对应的每一句古诗。由于生成的古诗的句数是固定的，因此所使用的关键词数目也也是固定的，如果关键词太多，则顺着前面选择，如果关键词太少，则需要进行扩充。扩充关键词可以根据额外的资源，比如维基百科，查找到相应的关键词，之后使用encoder-decoder 模型，以第一个关键词生成第一句，根据第一句和第二个关键词生成第二句，后面的每一句都使用对应的关键词以及前面生成的所有的诗，最终生成整首诗。

核心技术：关键词提取，关键词扩充，encoder-decoder模型，评价指标

【关键词提取】 ---- 论文[8]
---- 算法一
	--名称：TextRank，关键词提取，关键短语提取，摘要生成
	--工具：TextRank4ZH 用于自动从中文文本中提取关键词和摘要，基于 TextRank 算法，使用 Python 编写
	--来源：http://www.oschina.net/p/textrank4zh/
	--理论：Mihalcea R, Tarau P. TextRank: Bringing order into texts[C]. Association for Computational Linguistics, 2004.
		  ：https://my.oschina.net/letiantian/blog/351154
	--思想：一个句子的任何两个单词之间都存在一条无向无权边，根据此图计算每个单词的重要性，最重要的若干单词作为关键词
	--安装：pip install textrank4zh
---- 算法二
	--名称：快速自动提取关键词(RAKE)算法
	--工具：rake
	--来源：http://www.open-open.com/lib/view/open1421808449078.html
		  :http://python.jobbole.com/82230/
	--理论：M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd
	--备注：目前没有对中文的应用例子
		  ：

【关键词扩展】
----方法
		--维基百科知识辅助
		--自行搞定


【神经语言模型】
----程序：参考lstm-char-cnn-tensorflow-master 实现

实现步骤
1，制作数据集 训练集  测试集
2，搭建神经网络
3，训练 测试


[demo]
----github: practical_seq2seq

[Experiments]
----
	Input  : (?,26)  kwyword <= 3, context <= 23
	Output : (?,7)  or (?,26)  filled with zero
	model  : Input -> LSTM(3 x 1024) -> Output
	LSTM   : BasicLSTM
	decoder: Attention based or just embedding_rnn_seq2seq
	lr     : 0.0001
	epoch  : 100000
	dropout: 0.5
	loss   : seq2seq.sequence_loss


[Train Net]
----
	server : 40
	time   : 5.10 14:27
	input  : (?,26)
	output : (?,7)
	epoch  : 100000
	file   : quatrain/demo/train.py
	log    : out.log
	model  : embedding_attention_seq2seq
	ability: 821.238187  for keyword extract
	results: 模型有收敛的趋势，诗句中重复的词太多。效果不好。

----
	server : 41
	time   : 5.9 20:13
	input  : (?,26)
	output : (?,7)
	epoch  : 100000
	file   : quatrain/demo/train.py
	log    : out.log
	model  : embedding_rnn_seq2seq
	ability:  867.138663 
	results: 该模型下，loss越来越高，不收敛！
	finished!

	--------
	time   : 5.13 12:50
	input  : (?,26)
	output : (?,26)
	file   : quatrain/demo/train.py
	model  : embedding_attention_seq2seq
	results:


----
	server : 42
	time   : 5.10 14:27
	input  : (?,26)
	output : (?,26)
	epoch  : 100000
	file   : quatrain/demo/train.py
	log    : out.log
	model  : embedding_rnn_seq2seq
	ability: 有收敛的趋势，诗句中汉字重复的情况比40的模型有所减少！结果看起来还不错,
	模型被一不小心损坏了，覆盖了最后的结果


couplet
----------------------------------------------------------------------------
内容：使用与是词生成模型相同的模型，使用对联的上联 生成对联的下联
模型：LSTM（2 x 512） embedding_attention_seq2seq
数据：X = (?,31),Y = (?,31),batch = 32,epoch = 100000,EOS = 1,'EOS'
时间：





[TensorFlow Seq2Seq]     tf.nn.seq2seq.py
RNN encoder-decoder sequence-to-sequence model
http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/seq2seq.html
1,'basic_rnn_seq2seq', 
2,'embedding_attention_seq2seq', 
3,'embedding_rnn_seq2seq', 
4,'embedding_tied_rnn_seq2seq', 
5,one2many_rnn_seq2seq',
6,'tied_rnn_seq2seq',

----
    outputs, states = basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)
    ----
	encoder_inputs : are a list of tensors representing inputs to the encoder,A,B,C
	decoder_inputs : are tensors representing inputs to the decoder, GO, W, X, Y, Z
	cell           : is an instance of the models.rnn.rnn_cell.RNNCell ,GRUCell or LSTMCell
	               : rnn_cell provides wrappers to construct multi-layer cells
  Out:
 	outputs,states : lists of tensors of the same length as decoder_inputs 
 	outputs : outputs of the decoder in each time-step , W, X, Y, Z, EOS   
 	states  : represent the internal state of the decoder at every time-step          

  Mostly:
    output of the decoder at time t is fed back and becomes the input of the decoder at time t+1
----
	outputs, states = embedding_rnn_seq2seq(
    encoder_inputs, decoder_inputs, cell,
    num_encoder_symbols, num_decoder_symbols,
    output_projection=None, feed_previous=False)
    --
    In:
      1,encoder_inputs and decoder_inputs are integer-tensors that represent discrete values,embedded into a dense representation 
      2,num_encoder_symbols on the encoder side, and num_decoder_symbols on the decoder side,maximum number of discrete symbols
      3,feed_previous 
      	False: the decoder will use decoder_inputs tensors as provided
      	True : the decoder would only use the first element of decoder_inputs,the previous output of the encoder would be used,used for decoding translations in our translation model, but it can also be used during training
      4,output_projection
        not specified (None):  the outputs of the embedding model will be tensors of shape batch-size by num_decoder_symbols as they represent the logits for each generated symbol
----

