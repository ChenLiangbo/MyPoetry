              ============================================
             ||Chines Poetry Geberate Algorithim Research||
              ============================================

                              Author     : ChenLiangbo
                              started at : 2017.4.20

[Aim]
----poet style poem,Li Taibai
----Specific title of Song imbamic


=========================================================
[1]Chinese Poetry Generation with Recurrent Neural Networks
=========================================================
首先由用户给定的关键词生成第一句，然后由第一句话生成第二句话，由一，二句话生成第三句话，重复这个过程，直到诗歌生成完成。模型的模型由三部分组成：
  (1) Convolutional Sentence Model（CSM）：CNN模型，用于获取一句话的向量表示。
  (2) Recurrent Context Model(RCM)    RNN，根据历史生成句子的向量，输出下一个要生成句子的Context向量。
  (3) Recurrent Generation Model(RGM)：字符级别RNN，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。

【诗歌自动生成方法分类】--[2]
1，based on rules and templates
2，based on various genetic algorithms
3，based on various statistical machine translation，SMT，主要用于生成对联
4，based on summarization
5，deep learning methods 

【诗歌自动生成评价指标】--[2]
1，fluency，a poem must be read fluently
2，meaningful，a poem must intentionally convey some conceptual message that
   is meaningful under some interpretation
3，a poem must exhibit features that distinguishes it from non-poetic text
4，coherent，a poem should discuss some focused topics


=======================================================================
[2]Chinese Song Iambics Generation with Neural Attention-based Model
=======================================================================
基于attention的encoder-decoder框架,将历史已经生成的内容作为源语言序列，将下
一句要生成的话作为目标语言序列。需要用户提供第一句话，然后由第一句生成第二句
第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌
基于Attention机制配合,LSTM，可以学习更长的诗歌，同时在一定程度上，可以提高前后语义的连贯性

Sequence -> Encoder -> Hidden ->Decoder -> Sequence
(x1,x2)		LSTM   	   (h1,h2,)	(LSTM)		(h1,h2,..)
End of generation: Global Once,Global Always
Dataset: same as ref1
----train: 216 Beauty Yu,349 Butterflies Love Flowers
----test : 21,35
----evaluate: 18 experts
Measurements: Poetic,meaningful,fluency,related
Experiments:
----First Line as input 

===========================================================================
[3]Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema
===========================================================================
Main Ideas:
----Given intents,generate poems,iPoet
----polish,like human author modify poems

Input:Key Words  K = {k1,k2,..,} ki = {c1,c2,c3..,}
Output:Poem      
P = [c(1,1),c(1,2),...c(1,n),
     c(2,1),c(2,2),...c(2,n),
     ...			...
     c(m,1),c(m,2),...c(m,n)]

    n = 5 ,7
    m = 4, 8

Method
----Vectorize keyWords -> Vector (1,128)
----Encoder   RNN
----Decoder   RNN
----Polish    
----Stop      iterations = 10 or similarity of cosine < 0.5

Experiments
----Dataset : Tang Poems,Song Poems,61960  (5,7  -- 4,8)
----Intents : Title, Poems :Content
----Train   : 2000 , Valiations : 1000
----Loss    : corss entropy
----Conv 3x3, MaxPool,pad = 0
----batch   : 100
----lr      : 0.8

Measurement
----Computer
    ----Perplexity
    ----BELU
----Human
    ----Fluency
    ----Poetiness
    ----Coherence
    ----Meaning

====================================================================
[4]Chinese Poetry Generation with Planning based Neural Network
====================================================================
思想：根据写作意图生成每一行的关键字，根据关键字生成每一句

优点：
  （1）格式正确，写作主旨明确
  （2）写作意图可以是关键词，也可以是一句话，甚至还可以是一段文字描述
  （3）不仅可以从给定的诗词数据集中学习到知识，还可以从额外不相关的资源中学到知识
  （4）给定的关键词可以是现代汉语的没描述方式，而不是需要使用古汉语中专有的描述方式
  （5）给定某些关键信息，可以从维基百科提取关键词构成每一句的topic，然后据此生成每一句

实现：
  （1）使用poem planning model根据给定的关键词生成每一句的核心topic
  （2）根据每一句的topic一行一行的生成全诗
  （3）encoder-decoder model  with RNN

输入：作者给定的写作意图，可以是关键词，一句话，一段文章
过程：Poem Planning and Poem Generation

写作意图 -> 提取关键词  -> 关键词扩展  -> 每行主题 -> 每句诗

诗 N行 {l1,l2,l3,...,}
词 N个 {k1,k2,k3,...,}
对于Poem Generation Stage 每一句诗由当前关键词以及前面的诗级联作为输入生成
由此，每一句诗的生成都和关键词相关，同时也和前面所有的句子相关

[算法]
----Poem Planning
  （1）Keyword Extraction----TextRank algorithm 算法分析输入句子中每个词语的重要性
  （2）Keyword Expansion： query Q is too short to extract enough keywords
  ----Short:Recurrent Neural Network Language Model  predict the subsequent keywords according to the preceding sequence of keywords：argmaxk P(ki|k1:i−1) 
  ----For Long:Knowledge-based method ,extra knowledge sources,Given a keyword ki, the key idea of the method is to find some words that can best describe or interpret ki,
  use the encyclopedia entries as the source of knowledge to expand new keywords from ki

----Poem Generation
  (1)generated line by line. Each line is generated by taking the keyword specified by the Poem Planning model and all the preceding text as input
  (2)sequence-to-sequence mapping problem: the keyword specified by the Poem Planning model and the previously generated text of the poem
  (3)attention based RNN encoder-decoder:support multiple sequences as input


[Experiment]
--data: quatrain,4 x 5 or 4 x 7, from the Internet
--dataset:total 76,859 quatrains,2,000 poems for validation, 2,000 poems for testing
--segment:  CRF based word segmentation,calculate the TextRank score for every word,selected as the keyword for the line,xtract a sequence of 4 keywords for
every quatrain
--training: proposed attention based RNN enc-dec model
            vocabulary----6000
            embedding dimensionality----512,initialized by word2vec
            RNN units:512, initialized uniform distribution with support [-0.08,0.08]
            optimizer:AdaDelta algorithm
            minibatch:128
            final model is selected according to the perplexity on the validation se
--Evaluation:
    Poeticness
    Fluency
    Coherence
    Meaning
--Baselines
    SMT
    RNNLM
    RNNPG:RNN-based Poem Generator,standard RNNLM and then all the other lines are generated iteratively 
    ANMT:Attention based Neural Machine Translation method

--Results
    (1)compare to already exists poem generating system or method
    (2)compare to real poem by peot with this method,same subject

------------------------------------------------------------------------------
|I can not tell whatever is generated by machine from written by human poet!!!|
-------------------------------------------------------------------------------


===============================================
[5]Generating Topical Poetry 
===============================================
思想：首先制作一个比较大的词典，押韵词库，根据给定的主题，生成一系列相关的词，网络首先选择韵词作为每句的最后一个词，然后根此生成全诗

[Vocabulary]
--英文诗词，可以扩展到其它格式的诗词，根据音节提取出模式，给模式编码


[Topically Related Words and Phrases]
--scored list of 1000 words/phrases

[Choosing Rhyme Words]
--Strict Rhyme, CMU pronunciation dictionary
--Slant Rhyme
--Non-Topical Rhyming Words

[Constructing FSA of Possible Poems]
--choosing rhyme words
--encodes all word sequences 


[Path extraction through FSA with RNN]

[知乎，深度学习生成诗词综述]
http://qingmang.me/articles/-2609457114363059733

=====================================================================
[6]SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient
=====================================================================



=================================================================
[7]Efficient Estimation of Word Representations in Vector Space
=================================================================
目标：使用神经网络学习算法，根据语料库进行训练，将词映射到50-100维度的向量空间，将单词转换为连续的向量
单词向量化方法：神经网络语言模型-前馈网络

[Feedforward Neural Net Language Model]
--model:input, projection, hidden and output layers,在输入层，输入一个1xV向量，映射到N维，v是vocabulary大小，然后连接一个 N × D 的映射层，计算量过大，可以使用哈夫曼树编码方式对输出结果进行编码，将计算开支减小到一半


[Recurrent Neural Net Language Model] 卷及神经网络语言模型
--model :input, hidden and output layer


[New Log-linear Models] ---minimize computational complexity
----Continuous Bag-of-Words Model:CBOW
----Continuous Skip-gram Model

使用提出的两种模型对单词编码维向量的质量进行了对比，这种对比主要在于相似的单词编码之后再向量空间有比较大的相似性


===========================================================================================
[8]Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation
===========================================================================================
内容：人机对话，首先使用PMI生成名词作为对话的主要关键词，然后使用encoder-decoder model生成包含关键词的回复

主要思想：使用seq2BF模型代替seq2seq模型，主要使用前馈和反馈模型，根据第一个词生成回复的一句话，所选定的关键词会随机地出现在句子的任何地方

[Keyword Prediction]
--pointwise mutual information(PMI)：


======================================================================================
[13]Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation
=======================================================================================
内容：使用两个RNN来对序列数据进行编码解码处理，一个RNN将序列编码到固定长度的向量，另外一个RNN将向量解码到另一个序列，目的是计算在给定序列条件下另外一个序列发生的条件概率。训练网络最大化此条件概率，训练好的网络有两方面的应用，一是给定序列生成新的序列，另一个是判断两个序列的评分（相似度）
实验：英语-法语翻译，WMT’14 

[encoder]
X = (x1; x2; : : : ; xN) 
Y = (y1; y2; : : : ; yM)

每一个词都是序列，one-hot向量 -在词库上的表示，将词库排列，出现的地方为1，其余地方为0，非常稀疏
每个序列都嵌入到500维的向量空间
encoder包括1000个隐含单元

[decoder]
--直接接在encoder的隐含状态state上，最后使用softmax输出，将结果映射到了一个单词上

例子：
vocabulary_x = ['i',"am","a","student","teacher","doctor"]
vocabulary_y = ["是","我","一个","老师","医生","学生",]

word_x = ['student']  -> word_y = ["学生"]
x = [0,0,0,1,0,0] 
y = [0,0,0,0,0,1]

[model]
x -> embedding(512) ->encoder(RNN 1000) -> decoder(RNN 1000) -> softmax(arg max(yt)) 

句子的表示就谁生成的512维的向量在空间上的分布具有某种相关性


================================================================================
[14]Sequence to Sequence Learning with Neural Networks
================================================================================
内容：使用seq2seq模型实现翻译，LSTM 作为encoder和decoder，英语-法语翻译，数据集 WMT-14
贡献：
  (1) 序列到序列的语言模型
  (2) 将输入序列逆序可以对长句子有更好的翻译效果


==================================================================
[15]A Neural Conversational Model
===================================================================
[Main Idea] 
----machine conversation,
----predicting the next sentence given the previous sentence or sentences in a conversation,
----producing answers given by a probabilistic model trained to maximize the probability of the answer given some contex
----usage: machine translation, question/answering, and conversations


[Advantage]
----end-to-end and thus requires much fewer hand-crafted rules,the model can give a technicol answer
----generating fluent and accurate replies to conversations

dataset : IT helpdesk dataset,queries and reponses

[model] ---- seq2seq
----Input : 'ABC' + 'EOS'
----Output: 'WXYZ' + 'EOS'
----Model : 'ABC' -> 'WXYZ'


[dataset]
----One : closed-domain IT helpdesk troubleshooting dataset
----Two : open-domain movie transcript dataset,OpenSubtitles

[Experiments]
----IT Helpdesk Troubleshooting experiments
  LSTM(One layer,1024 units),SGD
  VOCABULARY : 20K words
  perplexity : 8 (n-gram:18)
  remember facts,understand contexts
  model can generalize to new question


======================================================================================
[16]Iterative Alternating Neural Attention for Machine Reading
======================================================================================
思想：使用可替代迭代机制来解决机器阅读理解任务，比如完形填空。完形填空的问题是在一个完整的句子当中删去一些词语生成的，所提出的模型被称为 neural attention-based inference model，模型使用一个RNN将数据读入，然后使用一种迭代机制寻找，document，query，word执念的关系，
数据可以描述为这样的关系对(Q; D; A; a)

[Alternating Iterative Attention]
encoding phase:  compute a set of vector representations
    X = (x1; : : : ; x|Xj)  document or query,vocabulary V
    Each word is represented by a continuous word embedding
    Encoder : GRU
inference phase: 寻求文档和query之间的关系以确保腿短结果有证可寻
iterative process：at each iteration, alternates attentive memory accesses to the 
    query and the document
prediction phase ： maximize the probability of the correct answer

[Experiments]
  batch_size: 32
  optimizer : SGD with Adam
  norm      : gradient > 5, gradient = 0.5*gradient
  lr        : lr = 0.8*lr if accuracy of validation not change
  embedding : 256 - 384
  decoder   : 128 - 256
  GRU       : 256 - 512
  dropout   : 0.2 of inference GRU

===========================================================================
[17]Neural Machine Translation by Joint Learning to Align and Translate
===========================================================================
内容：定长的序列模型限制了魔性的使用，所以文中使用不定长的序列来实现机器翻译，模型根据输入的不同，自动寻找与输出相关的部分，然后生成翻译的结果输出。模型在英语-法语翻译任务上进行得到测试。文中提出的模型是对传统encoder-decoder魔性的扩展，是模型具有联合翻译的能力。每一个时刻，模型产生一个单词输出的额时候，都会在输入的源句里面寻找最相关的信息，所以产生的输出不仅和当前已经生成的输出结果相关，还输入中的有关信息相关。
创意：模型的创意在于不是一次性间输入序列encode到一个固定长度的向量，而是将输入序列编码到一系列的向量序列中，在解码的时候从这一系列的序列向量中选择一部分，这样的方式可以解决原来模型中不管输入序列用没有用都必须一次性选择，然后对输出必有影响的局限。于是现在变成了，输出是收到输入序列影响的，但是这种影响的重要性被分成了很多小的子部分，在每次输出的时候收到什么成分影响，影响的大小如何完全由模型来自动决定。

[model]
encoder: a bidirectional RNN,这种RNN包括了前向和反向，前向的时候按照顺序读入输入序列，反向的时候将输入序列逆序读入，都计算隐含状态。
decoder: a decoder that emulates searching through a source sentence during decoding a translation (speech recognition perform well)

[Experiments]
--dataset : English-to-French translation,  ACL WMT ’14
--


=====================================================================================
[18]Generating Chinese Classical Poems with Statistical Machine Translation Models
=====================================================================================
内容：提出一种基于概率的方法生成中国古诗，以及一种评价古诗的方法，算法接收一系列给定的关键词，代表着写作意向，然后一句一句地顺着生成古诗，这个算法称为统计机器翻译。一个模型生成一句，使用一个额外的模型来加强句子之间的相关性，文中的算法时使用模板匹配生成第一句，然后使用统计翻译模型有第一句生成第二句，第二句生成第三句，第三句生成第四局。文中的写作意图也是是用几个关键字，这些关键词限制在《诗学含英》所归纳整理的大类中的词，根据关键词，根据诗歌中平平zhezhe对仗规律选择相应的词放在对应的位置上

模板匹配法：用户选定韵节词汇，给定关键词，于是系统从现有的词库中选取相应的词汇构成一首诗，这样生成的满足了押韵，但是没有一个中心，稻香老农系统辨识使用这样的方法生成古诗词的


==========================================================================
[19]Chameleons in imagined conversations: A new approach to understanding
coordination of linguistic style in dialogs
===========================================================================
[Main]